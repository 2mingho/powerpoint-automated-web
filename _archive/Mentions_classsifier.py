# -*- coding: utf-8 -*-
"""Report_Maker_G.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tf7t3NO6vJ_fuwuRmdk2J244YftPzwnG

#Import - Encoding and Display head to check if right
"""

import pandas as pd
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.ticker import FuncFormatter
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import textwrap
import os
from os import path
import requests

social_networks_prensa = ['@EdeesteRD', '@precisionportal', 'panoramatv_',
                          '@mananerord', '@noticiaspimentl', '@MIC_RD',
                          '@soldelamananard', '@drmariolama', '@labazucaenlared',
                          '@eldia_do', '@diariosaluddo',
                          '@noticiasrnn',
                          '@periodicohoy',
                          '@telemicrohd',
                          '@precisionportal',
                          '@telenoticiasrd',
                          '@sin24horas',
                          'sin24horas',
                          '@diariolibre',
                          'diariolibre',
                          '@cdn37',
                          'cdn.com.do',
                          '@listindiario',
                          'listindiario',
                          'bavaronews',
                          '@acentodiario',
                          '@anoticias7',
                          '@z101digital',
                          'z101digital',
                          'revista110',
                          'rcavada',
                          '@rcavada',
                          'robertocavada',
                          'ndigital',
                          'bavarodigital',
                          '@elnuevodiariord',
                          '@acentodiario',
                          '@deultimominutomedia',
                          '@aeromundo_ggomez',
                          '@cachichatv',
                          '@remolachanews',
                          '@ntelemicro5',
                          '@megadiariord',
                          '@noticiasmn',
                          '@eldemocratard',
                          '@elpregonerord',
                          '@infoturdom',
                          '@comunidadojala',
                          '@panorama_do', '@panoramard1',
                          '@invertix',
                          '@derechoasaberlo',
                          '@rccmediard', '@n16noticias', '@lospueblistas']

# # Reforma Fiscal
# keyword_database = {
#     "FMI":["fmi", "fondo monetaio internacional"],
#     "Jochi Vicente": ["jochi vicente", "jochi"],
#     "Pavel Isa Contreras": ["pavel contreras","pavelontreras", "pavel isa contreras","pavelisacontreras", "pavel"],
#     "HÃ©ctor Valdez Albizu": ["hector valdez","hectorvaldez", "hector valdez albizu", "hÃ©ctor valdÃ©z"],
#     "Pedro Silverio": ["pedro silverio","pedrosilverio"],
#     "Joel Santos": ["joel santos", "joel santo","joelsantos"],
#     "Pablo Portes": ["pablo portes", "pabloportes"],
#     "JosÃ© Rijo Presbot": ["jose rijo presbot", "jose rijo", "joserijo","josÃ© rijo"],
#     "Peter Prazmowski": ["peter prazmowski", "prazmowski", "peterprazmowski"],
#     "Mauricio DeVengochea": ["mauricio devengochea", "devengochea", "mauriciodevengochea"],
#     "Raquel PeÃ±a": ["raquel pena", "raquel peÃ±a", "raquelpena"],
#     "Antonio Tavares": ["antonio tavares", "atavares", "antoniotavares"],
#     "MagÃ­n DÃ­az": ["magin diaz", "magÃ­n dÃ­az", "magÃ­n", "magin"],
#     "Germania MontÃ¡s": ["germania montas", "germania montÃ¡s", "germaniamontas"],
#     "Andy Dauhajre": ["dauhajre", "andydauhajre", "andy dauhajre"],
#     "JosÃ© Lois Malkun": ["josÃ© lois malkun", "jose lois malkun", "joseloismalkun", "jose malkun"],
#     "Miguel Collado": ["miguel collado", "mcollado", "miguelcollado"],
#     "RaÃºl Ovalle": ["raul ovalle", "raÃºl ovalle", "raulovalle"],
#     "Haivanjoe NG CortiÃ±as": ["haivanjoe ng cortiÃ±as",'cortiÃ±as','ng cortiÃ±as'],
#     "Antonio Ciriaco": ["antonio ciriaco", "aciriaco", "antoniociriaco", "ciriaco marte"],
#     "Jaime Aristy Escuder": ["jaime aristy escuder", "jaime escuder", "aristy"],
#     "Rafael Espinal": ["rafael espinal", "rafaelespinal", "respinal"],
#     "Guarocuya Felix": ["guarocuya fÃ©lix", "guarocuya felix", "guarocuyafelix"],
#     "Franklyn VÃ¡squez": ["franklyn vÃ¡squez", "franklynvasquez", "franklyn vasquez"],
#     "Frederich E. BergÃ©s": ["frederich e. bergÃ©s", "frederich e bergÃ©s", "frederich bergÃ©s", "frederich e. berges", "frederichberges"],
#     "Luis Miguel Piantini": ["luis miguel piantini", "piantini", "luismiguelpiantini", "luis piantini"],
#     "Henri Hebrard": ["henri hebrard", "henrihebrard"],
#     "Isidro Santana": ["isidro santana", "isidrosantana"],
#     "Rolando GuzmÃ¡n": ["rolando guzmÃ¡n", "rolando guzman", "rolandoguzman"],
#     "FENACERD":["josÃ© dÃ­az", "jose dÃ­az", "josÃ© diaz", "jose diaz", "fenacerd"],
#     "CONACERD":["antonio cruz", "antoniocruz", "conacerd"],
#     "FDC":["ivÃ¡n garcÃ­a", "ivÃ¡n garcia", "ivan garcÃ­a", "ivan garcia","ivangarcia", "fdc"],
#     "Detallistas Unidos":["mario abreu henrÃ­quez", "mario abreu henriquez","mario abreu", "amaprosan"],
#     "ACIS":["sandyfilpo","sandy filpo", "acis"],
#     "CNCP":["marino calderÃ³n","marino calderon","marinocalderon", "alberto leroux", "albertoleroux","cncp"],
#     "FENACODEP":["faustofiguereo","fausto figuereo", "fenacodep"],
#     "CONEP":["conep_rd", "conep", "consejo nacional de la empresa privada"],
#     "Celso Marranzini":["celso", "marranzini", "celsomprp"],
#     "CÃ©sar Dargam":["cÃ©sar","cesar","dargam", "cesardargam"],
#     "Luis Abinader": ["abinader", "luis abinader"],
#     "Victor -Ito- BisonÃ³": ["ito", "ito bisonÃ³", "bisonÃ³", "victor bisono", "vÃ­ctor bisonÃ³"],
# }

# Roque Espaillat
# keyword_database = {
#     'Yadira Marte': ['yadira marte', '@yadiramarte3', 'yadira'],
#     'Ramfis Trujillo':['ranfi','ranfis','ramfis'],
#     'RamÃ³n Tolentino':['@ramontolentinot', 'ramÃ³n tolentino', 'ramon tolentino', 'tolentino'],
#     'Nuria':['nuria piera', '@ndigital', 'nuria','@nuriapiera'],
#     'Dr Fadul':['dr fadul','dr. fadul','fadul','@drfadul1'],
#     'JosÃ© Peguero':['josÃ© peguero', 'jose peguero', 'peguero', '@josepeguero'],
#     'Altagracia Salazar':['altagracia','altagracia salazar', '@altagraciasa', 'salazar', 'salasar'],
#     'Danny AlcÃ¡tara':['danny', 'alcÃ¡ntara', 'alcantara', 'danny alcantara', '@dn.alcantara', '@danyalcantarac', 'dany alcantara', 'dany alcÃ¡ntara'],
#     'Ariel Lara':['@ariellarardn6','ariel lara','ariel'],
#     'Grupo Propagas':['propagas','propagÃ¡s'],
#     'Guillermo Estrella':['guillermo estrella'],
#     'EnergÃ­a 2000':['empresas energÃ­a 2000','energÃ­a 2000','energia 2000'],
#     'Manzanillo energy':['manzanillo','manzanillo energy']
# }

# # Reforma Fiscal
# input_name_database = {
#     "Gobierno":["Luis Abinader","Jochi Vicente",
#                 "Pavel Isa Conde","HÃ©ctor Valdez Albizu",
#                 "Pedro Silverio","Victor -Ito- BisonÃ³",
#                 "Joel Santos","Pablo Portes",
#                 "JosÃ© Rijo Presbot","Peter Prazmowski",
#                 "Mauricio DeVengochea","Raquel PeÃ±a", "Antonio Tavares"],
#     "Economistas":["MagÃ­n DÃ­az", "Germania MontÃ¡s","Andy Dauhajre",
#                    "JosÃ© Lois Malkun","Miguel Collado","RaÃºl Ovalle",
#                    "Haivanjoe NG CortiÃ±as","Antonio Ciriaco","Jaime Aristy Escuder",
#                    "Rafael Espinal","Guarocuya Felix","Franklyn VÃ¡squez",
#                    "Frederich E. BergÃ©s","Luis Miguel Piantini","Henri Hebrard",
#                    "Isidro Santana","Rolando GuzmÃ¡n"],
#     "Federaciones de Comerciantes":["FENACERD","CONACERD",
#                                     "FDC","Detallistas Unidos",
#                                     "ACIS","CNCP","FENACODEP"],
#     "CONEP":["CONEP", "Celso Marranzini", "CÃ©sar Dargam"],
#     "FMI":["FMI"],
# }

# Specify the path to your CSV file
file_path = input("Inserte la ruta: ")  # Replace 'your_data.csv' with your actual file name.

# Read the CSV with UTF-16 encoding and Tab separator
df = pd.read_csv(file_path, encoding='utf-16', sep='\t')

# Display a few initial rows to check the import
#display(df)

display(df.columns)

"""#Duplicate the file to make changes in a Copy
Delete the unused rows based on a list
"""

# List of columns to delete
columns_to_delete = [
    'Opening Text', 'Subregion', 'Desktop Reach', 'Mobile Reach',
    'Twitter Social Echo', 'Facebook Social Echo', 'Reddit Social Echo',
    'National Viewership', 'Engagement', 'AVE', 'State', 'City',
    'Social Echo Total', 'Editorial Echo', 'Views', 'Estimated Views',
    'Likes', 'Replies', 'Retweets', 'Comments', 'Shares', 'Reactions',
    'Threads', 'Is Verified'
]

# Create a copy of the DataFrame for cleaning
df_cleaned = df.copy()

# Drop the unnecessary columns from the copy
df_cleaned = df_cleaned.drop(columns_to_delete, axis=1, errors='ignore')

# Verify changes (optional)
display(df_cleaned.columns)

"""# ðŸŽ Change KEYwORD column value"""

def update_column(df_cleaned, column_name, new_value):
    """
    Modifica todos los valores de una columna especÃ­fica en un DataFrame.

    Args:
        df_cleaned: El DataFrame a modificar.
        column_name: El nombre de la columna a actualizar.
        new_value: El nuevo valor para todos los elementos de la columna.

    Returns:
        El DataFrame modificado.
    """

    # Verificar si la columna existe en el DataFrame
    if column_name not in df_cleaned.columns:
        print(f"Error: La columna '{column_name}' no existe en el DataFrame.")
        return df_cleaned  # Devolver el DataFrame original sin cambios

    # Actualizar la columna
    df_cleaned[column_name] = new_value

    return df_cleaned

# Ejemplo de uso:
# Supongamos que quieres cambiar todos los valores de la columna 'Sentiment' a 'Neutral'
# df_cleaned = update_column(df, 'Keywords', 'Bebidas')

# FunciÃ³n para clasificar automÃ¡ticamente los 'Keywords'
def clasificar_keywords(df, keyword_database):
    """
    Clasifica automÃ¡ticamente los 'Keywords' basados en 'Hit Sentence'.
    """
    for index, row in df.iterrows():
        hit_sentence = row['Hit Sentence'].lower()
        for keyword, phrases in keyword_database.items():
            if any(phrase in hit_sentence for phrase in phrases):
                df.at[index, 'Twitter Authority'] = keyword
                break
    return df

def clasificar_input_name(df, input_name_database):
    """
    Clasifica automÃ¡ticamente los 'Input Name' basados en 'Twitter Authority'.
    """
    for index, row in df.iterrows():
        authority = row['Twitter Authority']
        for input_name, authorities in input_name_database.items():
            if authority in authorities:  # Verificar si el nombre estÃ¡ en la lista de autoridades
                df.at[index, 'Input Name'] = input_name
                break
        else:  # Si no se encuentra coincidencia en ninguna categorÃ­a
            df.at[index, 'Input Name'] = row['Input Name']
    return df

def update_or_classify_keywords(df, keyword_database=None):
    """
    Actualiza la columna 'Keywords' en el DataFrame df. Permite elegir entre actualizaciÃ³n manual o automÃ¡tica.

    Args:
        df (pandas.DataFrame): El DataFrame a modificar.
        keyword_database (dict, optional): Diccionario de palabras clave para clasificaciÃ³n automÃ¡tica.
                                           Si no se proporciona, la actualizaciÃ³n serÃ¡ manual.

    Returns:
        pandas.DataFrame: El DataFrame modificado.
    """

    while True:
        opcion = input("Â¿Desea ajustar 'Keywords' manualmente (m) o automÃ¡ticamente (a)? ").lower()
        if opcion in ['m', 'a']:
            break
        else:
            print("OpciÃ³n invÃ¡lida. Por favor, ingrese 'm' o 'a'.")

    if opcion == 'm':
        new_keywords = input("Ingrese los nuevos 'Keywords' (separados por comas): ")
        df = update_column(df, 'Keywords', new_keywords)  # Usamos la funciÃ³n update_column ya definida
    elif opcion == 'a':
        if keyword_database is None:
            print("Error: No se proporcionÃ³ un diccionario de palabras clave para la clasificaciÃ³n automÃ¡tica.")
            return df  # Devolver el DataFrame original sin cambios

        df = clasificar_keywords(df, keyword_database)

    return df

# Llamamos a la funciÃ³n para actualizar o clasificar la columna 'Keywords'
df_cleaned = update_or_classify_keywords(df_cleaned, keyword_database)
# Llama a la funciÃ³n para clasificar
df_updated = clasificar_input_name(df_cleaned, input_name_database)

"""---"""

display(df_cleaned[['Hit Sentence','Keywords','Input Name','Twitter Authority']])

"""#Set the right 'Hit Sentence' content on news or blogs"""

# Update Hit Sentence with Headline if Headline is not empty
df_cleaned['Hit Sentence'] = df_cleaned['Headline'].where(~df_cleaned['Headline'].isna(), df_cleaned['Hit Sentence'])

# Print some rows to verify the change (optional)
# display(df_cleaned)

"""#Fix the 'Source'/'Influencer' columns"""

# def if_empty_influencer(row):
#   if pd.isna(row['Source']) or pd.isnull(row['Source']):
#     return row['URL'].split('/')[2]

# df_cleaned['Source'] = df_cleaned.apply(if_empty_influencer, axis=1)
# display(df_cleaned.head())

# Define the list of sources to exclude
exclude_sources = ['Blogs', 'Twitter', 'Youtube', 'Instagram', 'Facebook', 'Pinterest', 'Reddit', 'TikTok', 'Twitch']

# Create a mask to select where the condition is met
mask = ~df_cleaned['Source'].isin(exclude_sources)

# Copy the 'Source' to 'Influencer' for the selected rows
df_cleaned.loc[mask, 'Influencer'] = df_cleaned.loc[mask, 'Source']

# Verify the change (optional)
# display(df_cleaned)

"""#Create the 'Plataforma' column and set the correct values based on hard coding"""

# Define a dictionary to map source names to platform categories
social_network_sources = {
    'Twitter': 'Redes Sociales',
    'Youtube': 'Redes Sociales',
    'Instagram': 'Redes Sociales',
    'Facebook': 'Redes Sociales',
    'Pinterest': 'Redes Sociales',
    'Reddit': 'Redes Sociales',
    'TikTok': 'Redes Sociales',
    'Twitch': 'Redes Sociales',
}

# Create a new 'Plataforma' column with default value 'Prensa Digital'
df_cleaned['Plataforma'] = 'Prensa Digital'

# Update 'Plataforma' for social network sources using a mapping function
def categorize_platform(source):
  return social_network_sources.get(source, 'Prensa Digital')  # Default to 'Prensa Digital' if not a social network

df_cleaned['Plataforma'] = df_cleaned['Source'].apply(categorize_platform)

"""#Fix the Facebook comments' name on the 'Influencer' column"""

def update_influencer(row):
    if row['Source'] == 'Facebook' and (row['Reach'] == 0 or pd.isna(row['Reach'])):
        return "Comment on " + row['Influencer']
    else:
        return row['Influencer']

def update_sentiment(row):
  if row['Sentiment'] == "Unknown" or pd.isna(row['Sentiment']):
    return "Neutral"
  else:
    return row['Sentiment']

df_cleaned['Influencer'] = df_cleaned.apply(update_influencer, axis=1)
df_cleaned['Sentiment'] = df_cleaned.apply(update_sentiment, axis=1)

"""---"""

def filtrar_sentimientos(df):
    """
    Filtra y ajusta los sentimientos en un DataFrame.

    Args:
        df (pandas.DataFrame): El DataFrame a procesar.

    Returns:
        pandas.DataFrame: El DataFrame filtrado y modificado.
    """
    # MÃ¡scara booleana para condiciones combinadas (se incluye la condiciÃ³n de 'Comunidad' == 'Medios')
    mask_prensa_blogs = (df['Influencer'].isin(social_networks_prensa)) | (df['Plataforma'] == 'Prensa Digital')

    # AsignaciÃ³n vectorial optimizada (se usa la mÃ¡scara actualizada)
    df['Sentiment'] = np.where(
        mask_prensa_blogs, "Neutral", df['Sentiment'].where(df['Sentiment'].isin(["Positive", "Negative", "Neutral"]), "Neutral")
    )

    return df

# Filter rows by sentiment
df_cleaned = filtrar_sentimientos(df_cleaned)

# Display the first 5 rows of the updated DataFrame
print(df_cleaned.head().to_markdown(index=False, numalign="left", stralign="left"))

display(df_cleaned.where(df_cleaned['Source']=='Facebook'))

"""#Calculate and return:
* Total mentions
* Authors
* Pot Reach
"""

# Total of Mentions
total_mentions = len(df_cleaned)

# Count of Unique Authors
count_of_authors = df_cleaned['Influencer'].nunique()

# Estimated Reach (sum of max reach per influencer)
influencer_groups = df_cleaned.groupby('Influencer')['Reach']
estimated_reach = influencer_groups.max().sum()

def format_number(number):
  """Formats a number to display as millions (M) or thousands (k)"""
  if number >= 1000000:
    return f"{number / 1000000:.1f}M"
  elif number >= 1000:
    return f"{number / 1000:.1f}k"
  else:
    return number

# Format the estimated reach
formatted_estimated_reach = format_number(estimated_reach)

# Print the results (including the formatted estimated reach)
print("Total Of Mentions:", total_mentions)
print("Count of Authors:", count_of_authors)
print("Estimated Reach:", formatted_estimated_reach)

"""##Chart Just by Date
***(Weekly reports)***

#ADD TIMESTAMPS [pending]
"""

date_column = 'Date'
time_column = 'Time'

# Extract date and hour components
def extract_date(datetime_str):
    try:
        return pd.to_datetime(datetime_str, format='%d-%b-%Y').date()
    except ValueError:
        # Handle cases where only the date is present
        return pd.to_datetime(datetime_str.split()[0], format='%d-%b-%Y').date()

def extract_hour(datetime_str):
    try:
        return pd.to_datetime(datetime_str, format='%d-%b-%Y %I:%M%p').strftime('%I %p')
    except ValueError:
        # Handle cases where only the time is present with extra space
        return pd.to_datetime(datetime_str.strip(), format='%I:%M %p').strftime('%I %p') # Modified format string

df_cleaned['date'] = df_cleaned[date_column].apply(extract_date)
df_cleaned['hour'] = df_cleaned[date_column] + ' ' + df_cleaned[time_column].apply(extract_hour)

def plot_by_date_or_hour(df, use_hours=True):
    # Group based on the selected strategy
    if use_hours:
        df_grouped = df.groupby(['date', 'hour']).size().reset_index(name='count')
        # x_axis_label = 'Hour of Day'
    else:
        df_grouped = df.groupby(['date']).size().reset_index(name='count')
        # x_axis_label = 'Date'

    # Prepare data for plotting (common to both scenarios)
    dates = df_grouped['date']
    count = df_grouped['count']

    # Plotting
    fig, ax = plt.subplots(figsize=(10, 5))

    # Plot for the selected date
    ax.plot(dates, count, linewidth=4, color='orange')

    # Remove top and right spines
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    # Custom for Altice report
    # ax.spines['bottom'].set_color('w')
    # ax.spines['left'].set_color('w')

    # # Axis colors
    # ax.yaxis.label.set_color('w')
    # ax.xaxis.label.set_color('w')
    # ax.tick_params(axis='x', colors='w')
    # ax.tick_params(axis='y', colors='w')

    # X-axis customization (common)
    ax.xaxis.set_major_locator(mdates.AutoDateLocator())
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%b-%Y'))
    plt.xticks(rotation=45, ha='right')

    plt.tight_layout()
    plt.savefig('convEvolution.png', transparent=True)
    plt.show()

# Usage:
plot_by_date_or_hour(df_cleaned, use_hours=False) # Plot by date only

"""#Word Cloud (based on Key Phrases)"""

# Function to process the 'Keywords' column with enhancements
def process_keywords(df):
    all_keywords = []
    # for keywords_str in df['Hit Sentence']:
    #     if str(keywords_str) != 'nan':  # Check for 'nan'
    #         words = keywords_str.split(" ")
    #         all_keywords.extend(words)

    for keyphrases_str in df['Key Phrases']:
        if str(keyphrases_str) != 'nan':  # Check for 'nan'
            words = str(keyphrases_str).split(",")
            all_keywords.extend(words)

    return ' '.join(filter(None, all_keywords * 2))  # Repeat keywords for emphasis

# Process the data and create the word cloud
text = process_keywords(df_cleaned)

mask_url = "https://dockship-images.s3.ap-south-1.amazonaws.com/afa5d11974963a8e6ec4ec2e0724beeb"
try:
    response = requests.get(mask_url, stream=True)
    response.raise_for_status()  # Check for HTTP errors
    cloud_mask = np.array(Image.open(response.raw).convert("RGB"))
except requests.exceptions.RequestException as e:
    cloud_mask = np.array(Image.open("/content/comment.png"))
    print(f"Error fetching mask from URL: {e}")


# Customizable stopwords
stopwords = set(STOPWORDS)

# Generate word cloud with modifications
wordcloud = WordCloud(stopwords=stopwords, background_color='white', mask=cloud_mask,
                      contour_width=0, min_font_size=14  # Minimum font size
                      # rotation=0)  # Force horizontal orientation
                    ).generate(text)

# Rotate after generation
image = Image.fromarray(wordcloud.to_array())
rotated_image = image.rotate(0, expand=True)  # Rotate 90 degrees, expand canvas
plt.imshow(rotated_image, interpolation='bilinear')
plt.axis("off")
plt.savefig('wordCloud.png', transparent=True)
plt.show()

"""---"""

def concatenar_key_phrases(df):
    """
    Concatena todos los valores de la columna 'Key Phrases' en un solo texto, eliminando puntos y comas.

    Args:
        df (pandas.DataFrame): El DataFrame que contiene la columna 'Key Phrases'.

    Returns:
        str: Una cadena de texto con todos los valores de 'Key Phrases' concatenados y limpios.
    """
    # Combine all values from the 'Key Phrases' column into a single string
    all_key_phrases = ",".join(df['Key Phrases'].astype(str))

    # Remove all occurrences of "." and ","
    texto_key_phrases = all_key_phrases.replace(".", " ").replace(",", " ")

    return texto_key_phrases

print(concatenar_key_phrases(df_cleaned))

"""#Sentiment Pie Chart"""

# Step 1: Count Sentiment Occurrences
sentiment_counts = df_cleaned[df_cleaned['Sentiment'] != 'Not Rated']['Sentiment'].value_counts()

# Color mapping for sentiments
sentiment_colors = {
    'Negative': '#ff0000',
    'Positive': '#00b050',
    'Neutral': '#7f7f7f'
}

# Step 2: Prepare Data for Pie Chart
labels = sentiment_counts.index.to_list()  # Extract sentiment categories as labels
sizes = sentiment_counts.to_list()  # Extract counts as slice sizes

# Create the Pie Chart
plt.figure(figsize=(7, 7))

# Apply colors from the color mapping
colors = [sentiment_colors.get(label, 'lightgray') for label in labels]

# Formatting arguments for pie chart
wedgeprops = dict(width=0.6, edgecolor='w', linewidth=1)  # Adjust wedge width and edge

# *** Key Changes for Transparency ***
plt.pie(
    sizes,
    autopct="%1.1f%%",
    colors=colors,
    textprops=dict(backgroundcolor='w', fontsize=10, fontweight='bold'),
    # wedgeprops=wedgeprops,  # Use wedgeprops for the customizations
    startangle=140  # Optional: Adjust the starting angle for visual appeal
)

plt.axis('equal')

# Save as Transparent PNG
plt.savefig('sentiment_pie_chart.png', transparent=True)  # Set transparent=True

plt.show()  # You can still show the plot if desired

"""#Platform Graph

---
"""

def distribucion_plataforma(df):
    """
    Calcula la distribuciÃ³n de plataformas, el nÃºmero de publicaciones y el alcance mÃ¡ximo por plataforma.

    Args:
        df (pandas.DataFrame): El DataFrame que contiene los datos.

    Returns:
        pandas.DataFrame: Un DataFrame con la distribuciÃ³n de plataformas, publicaciones y alcance mÃ¡ximo.
    """
    # Count the frequency of each value in the 'Plataforma' column
    platform_counts = df['Plataforma'].value_counts()

    # Group by 'Plataforma' and calculate the number of posts and max reach
    posts_per_platform = df.groupby('Plataforma')['Date'].count()
    max_reach_per_platform = df.groupby('Plataforma')['Reach'].max()

    # Create a DataFrame with the platform distribution
    platform_distribution = pd.DataFrame({
        'Plataforma': platform_counts.index,
        'Publicaciones': platform_counts.values,
        'Alcance MÃ¡ximo': max_reach_per_platform.values
    })

    # Format the 'Alcance MÃ¡ximo' column
    platform_distribution['Alcance MÃ¡ximo'] = platform_distribution['Alcance MÃ¡ximo'].apply(format_number)

    return platform_distribution


# Apply the function to get the platform distribution
platform_distribution_df = distribucion_plataforma(df_cleaned)

# Print the platform distribution DataFrame
display(platform_distribution_df)

"""Top Users By platform"""

# Create filters
filter_prensa_digital = df_cleaned['Plataforma'] == "Prensa Digital"
filter_redes_sociales = df_cleaned['Plataforma'] == "Redes Sociales"

# Formatting function with integer conversion
def format_reach(number):
    if pd.isna(number):  # Check for NaN
        return number  # Keep NaN as-is
    else:
        return f"{int(number):,}"  # Convert to int and format

print("Redes Sociales (By Posts)")

# Table 2: 'Redes Sociales' influencers
df_redes = df_cleaned[filter_redes_sociales]  # Filter for 'Redes Sociales'

# 1. Corrected Aggregation:
#   - Include 'Source' in the groupby and select it in the result.
#   - Use the 'first' aggregation for 'Source' since it's expected to be the same for a given influencer.
#   - Rename columns for clarity
df_redes_grouped = (
    df_redes.groupby('Influencer')[['Reach', 'Source']]
    .agg(Posts=('Reach', 'count'), Max_Reach=('Reach', 'max'), Source=('Source', 'first'))
)

# 2. Formatting:
df_redes_grouped['Max_Reach'] = df_redes_grouped['Max_Reach'].apply(format_reach)

# 3. Sorting and Displaying:
df_redes_grouped = df_redes_grouped.sort_values(by='Posts', ascending=False).head(15)

display(df_redes_grouped[['Posts', 'Max_Reach', 'Source']])

display(df_redes_grouped)

# Create filters
filter_prensa_digital = df_cleaned['Plataforma'] == "Prensa Digital"
filter_redes_sociales = df_cleaned['Plataforma'] == "Redes Sociales"

# Formatting function with integer conversion
def format_reach(number):
    if pd.isna(number):  # Check for NaN
        return number  # Keep NaN as-is
    else:
        return f"{int(number):,}"  # Convert to int and format

print("Prensa Digital (By Posts)")
# Table 1: 'Prensa Digital' influencers
df_prensa = df_cleaned[filter_prensa_digital]  # Filter for 'Prensa Digital'
df_prensa_grouped = df_prensa.groupby('Influencer')[['Reach']].agg(['count', 'max'])
df_prensa_grouped.columns = ['Posts', 'Max Reach']

df_prensa_grouped = df_prensa_grouped.sort_values(by='Posts', ascending=False).head(15)
# Apply formatting to 'Max Reach'
df_prensa_grouped['Max Reach'] = df_prensa_grouped['Max Reach'].apply(format_reach)

# Sort and show top 15 (descending order for the first table)
display(df_prensa_grouped)

# print("\n------------------------\n") Â # Divider between tables

print("Redes Sociales (By Posts)")
# Table 2: 'Redes Sociales' influencers
df_redes = df_cleaned[filter_redes_sociales]  # Filter for 'Redes Sociales'
df_redes_grouped = df_redes.groupby('Influencer')[['Reach']].agg(['count', 'max'])
df_redes_grouped.columns = ['Posts', 'Max Reach']

# Apply formatting to 'Max Reach' in the second table
df_redes_grouped['Max Reach'] = df_redes_grouped['Max Reach'].apply(format_reach)

# Sort and show top 15
df_redes_grouped = df_redes_grouped.sort_values(by='Posts', ascending=False).head(15)

display(df_redes_grouped)

# print("\n------------------------\n") Â # Divider between tables

print("Redes Sociales (By Reach)")
# Filter 'Redes Sociales'
df_redes = df_cleaned[filter_redes_sociales]

# Group by 'Influencer'
df_redes_grouped = df_redes.groupby('Influencer')[['Reach']].agg(['count', 'max'])
df_redes_grouped.columns = ['Posts', 'Max Reach']

# Sort by 'Max Reach' descendente, then by 'Posts' descendente
df_redes_grouped = df_redes_grouped.sort_values(by='Max Reach', ascending=False).head(15)

# Format 'Max Reach'
df_redes_grouped['Max Reach'] = df_redes_grouped['Max Reach'].apply(format_reach)

# Reorder columns
df_redes_grouped = df_redes_grouped[['Max Reach', 'Posts']]

# Show top 10
display(df_redes_grouped)

"""#Top news"""



# Filter for 'Prensa Digital'
df_prensa = df_cleaned[df_cleaned['Plataforma'] == "Prensa Digital"]

# Sort by 'Reach' (descending) and select top 5 influencers based on max reach
top_influencers = df_prensa.sort_values(by=['Reach'], ascending=False) \
                             .groupby('Influencer')['Reach'].max() \
                             .head(10).index

# Extract the corresponding 'Hit Sentence' values
top_5_hit_sentences = df_prensa[df_prensa['Influencer'].isin(top_influencers)]['Hit Sentence']

for sentence in top_5_hit_sentences:
    wrapper = textwrap.TextWrapper(width=100)  # Set desired line width
    wrapped_text = wrapper.wrap(text=sentence)
    for line in wrapped_text:
        print(line)

"""#Export as csv"""

# Obtener el nombre base del archivo original (antes del primer espacio)
base_filename = os.path.basename(file_path).split()[0]
base_filename = base_filename.split('.')[0]

# Construir el nuevo nombre del archivo
output_filename = f"{base_filename} (df_cleaned).csv"

# Guardar el DataFrame en el archivo renombrado
with open(output_filename, 'w', encoding='utf-16', newline='') as f:
    df_cleaned.to_csv(f, index=False, sep='\t')

print(f"CSV data saved to: {output_filename}")

"""# MERGE FILES"""

def merge_csv_files():
    """
    Une varios archivos CSV en uno solo, verificando la estructura de columnas,
    codificaciÃ³n UTF-16 y separador de tabulaciones.
    """

    num_files = int(input("Â¿CuÃ¡ntos archivos CSV deseas unir? "))
    file_paths = []

    for i in range(num_files):
        file_path = input(f"Ingresa la ruta del archivo CSV #{i+1}: ")
        file_paths.append(file_path)

    # Leer el primer archivo para obtener la estructura de columnas de referencia
    df_reference = pd.read_csv(file_paths[0], encoding='utf-16', sep='\t')
    # df_reference = pd.read_csv(file_paths[0], encoding='utf-8', sep=',')

    # DataFrame para almacenar los datos combinados
    merged_df = pd.DataFrame(columns=df_reference.columns)

    # Iterar sobre los archivos y unirlos si tienen la misma estructura
    for file_path in file_paths:
        df = pd.read_csv(file_path, encoding='utf-16', sep='\t')  # Leer con UTF-16 y tab
        # df = pd.read_csv(file_path, encoding='utf-8', sep=',')  # Leer con UTF-8 y tab

        # Verificar si la estructura de columnas es la misma
        if list(df.columns) != list(df_reference.columns):
            print(f"Error: El archivo '{file_path}' tiene una estructura de columnas diferente.")
            continue

        # Unir los datos al DataFrame combinado
        merged_df = pd.concat([merged_df, df], ignore_index=True)

    # Solicitar el nombre del archivo de salida
    output_filename = input("Ingresa el nombre del archivo CSV de salida (o presiona Enter para usar 'merged_data.csv'): ")
    if not output_filename:
        output_filename = "merged_data.csv"
    else:
        output_filename = output_filename + '.csv'

    # Guardar el DataFrame combinado en un archivo CSV con UTF-16 y tab
    merged_df.to_csv(output_filename, index=False, encoding='utf-16', sep='\t')
    # merged_df.to_csv(output_filename, index=False, encoding='utf-8', sep=',')

    print(f"Archivos CSV unidos y guardados en: {output_filename}")

# Ejecutar la funciÃ³n
merge_csv_files()

# text = "http://elmundodelosnegocios.com.do/v1/index.php/2024/06/04/el-hospital-germans-trias-i-pujol-de-espana-pone-en-marcha-un-programa-de-telesalud-y-gestion-remota-de-pacientes-mediante-relojes-medicos-masimo-w1-y-monitores-de-constantes-vitales-ponibles-ra/"

# text_sep = text.split('/', 3)

# print(text_sep)

# import pandas as pd
# from datetime import datetime, timedelta
# import locale
# import dateparser

# def convertir_fecha(texto_fecha):
#     try:
#         fecha = dateparser.parse(texto_fecha, languages=['es'], settings={'RELATIVE_BASE': datetime.now()})
#         return fecha.strftime("%d-%m-%Y") if fecha else None
#     except ValueError:
#         return None

# # ConfiguraciÃ³n regional para espaÃ±ol (importante para interpretar "mes" y "dÃ­a")
# locale.setlocale(locale.LC_TIME, 'es_DO.UTF-8')

# # Leer el archivo CSV (ajusta 'tu_archivo.csv' con la ruta correcta)
# df = pd.read_csv('/content/merged_data.csv', encoding='utf-8-sig', sep=',')

# # FunciÃ³n para convertir "X dÃ­as/meses atrÃ¡s" a fecha DD-MM-YY
# def convertir_fecha(texto_fecha):
#     try:
#         numero, unidad = texto_fecha.split()[:2]  # Extraer nÃºmero y unidad
#         numero = int(numero)

#         if unidad.startswith('dÃ­a'):
#             fecha = datetime.now() - timedelta(days=numero)
#         elif unidad.startswith('mes'):
#             fecha = datetime.now() - timedelta(days=numero * 30)  # AproximaciÃ³n
#         else:
#             raise ValueError(f"Unidad de tiempo desconocida: {unidad}")

#         return fecha.strftime("%d-%m-%Y")

#     except ValueError:
#         return None  # Manejar fechas invÃ¡lidas

# # Aplicar la conversiÃ³n a la columna 'publishedTimeText'
# df['publishedTimeText'] = df['publishedTimeText'].astype(str).apply(convertir_fecha)

# # Guardar el archivo CSV modificado
# df.to_csv('tu_archivo_convertido.csv', encoding='utf-8-sig', sep=',', index=False)

# print("ConversiÃ³n de fechas completada. Resultados guardados en 'tu_archivo_convertido.csv'")

# !pip install dateparser

# !pip install transformers

# import pandas as pd
# from textblob import TextBlob
# from google.colab import files

# # # Subir el archivo CSV a Google Colab
# # uploaded = files.upload()

# # # Obtener el nombre del archivo subido
# # for filename in uploaded.keys():
# #     nombre_archivo = filename

# # Leer el archivo CSV (ajusta la ruta)
# df = pd.read_csv("/content/merged_data (1).csv",lineterminator='\n')


# # FunciÃ³n para clasificar el sentimiento (usando TextBlob)
# def clasificar_sentimiento(texto):
#     analysis = TextBlob(texto)
#     polaridad = analysis.sentiment.polarity

#     if polaridad > 0:
#         return "Positivo"
#     elif polaridad < 0:
#         return "Negativo"
#     else:
#         return "Neutral"

# # Aplicar la clasificaciÃ³n y crear la nueva columna (dentro del bloque try)
# df['votes'] = df['simpleText'].astype(str).apply(clasificar_sentimiento)

# # Guardar el archivo CSV modificado
# df.to_csv('tu_archivo_clasificado.csv', encoding='utf-8-sig', sep=',', index=False)

# # Descargar el archivo clasificado
# files.download('tu_archivo_clasificado.csv')

# print("AnÃ¡lisis de sentimiento completado. Resultados descargados.")

# import pandas as pd
# from textblob import TextBlob
# from google.colab import files

# # Subir el archivo CSV a Google Colab
# uploaded = files.upload()

# # Obtener el nombre del archivo subido
# for filename in uploaded.keys():
#     nombre_archivo = filename

# # Leer el archivo CSV (ajusta la ruta)
# try:
#     df = pd.read_csv(nombre_archivo, encoding='utf-16', sep='\t')
# except pd.errors.ParserError as e:
#     print(f"Error al leer el archivo CSV: {e}")
#     # AquÃ­ puedes agregar lÃ³gica para manejar el error, como intentar leer con otra codificaciÃ³n o delimitador
#     exit()  # Salir del script si no se puede leer el archivo

# # Columna que quieres analizar
# columna_texto = 'Hit Sentence'

# # FunciÃ³n para clasificar el sentimiento (usando TextBlob)
# def clasificar_sentimiento(texto):
#     analysis = TextBlob(texto)
#     polaridad = analysis.sentiment.polarity

#     if polaridad > 0:
#         return "Positivo"
#     elif polaridad < 0:
#         return "Negativo"
#     else:
#         return "Neutral"

# # Aplicar la clasificaciÃ³n y crear la nueva columna
# df['Sentimiento'] = df[columna_texto].astype(str).apply(clasificar_sentimiento)

# # Guardar el archivo CSV modificado
# df.to_csv('tu_archivo_clasificado.csv', encoding='utf-16', sep='\t', index=False)

# # Descargar el archivo clasificado
# files.download('tu_archivo_clasificado.csv')

# print("AnÃ¡lisis de sentimiento completado. Resultados descargados.")

"""# Train Transformer"""

# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
# from datasets import Dataset

# # ... (cargar datos etiquetados en un DataFrame df)

# # Convertir a formato Dataset de Hugging Face
# dataset = Dataset.from_pandas(df)

# # Cargar modelo y tokenizer
# model_name = "distilbert-base-uncased-finetuned-sst-2-english"
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForSequenceClassification.from_pretrained(model_name)

# # Tokenizar los datos
# def tokenize_function(examples):
#     return tokenizer(examples["Hit Sentence"], padding="max_length", truncation=True)

# tokenized_datasets = dataset.map(tokenize_function, batched=True)

# # Definir argumentos de entrenamiento
# training_args = TrainingArguments(
#     output_dir="./results",
#     evaluation_strategy="epoch",
#     per_device_train_batch_size=16,
#     per_device_eval_batch_size=64,
#     num_train_epochs=3,
#     weight_decay=0.01,
# )

# # Crear el Trainer
# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=tokenized_datasets["train"],
#     eval_dataset=tokenized_datasets["test"],
# )

# # Entrenar el modelo
# trainer.train()

# import requests
# from bs4 import BeautifulSoup

# def get_twitter_profile_picture(username):
#     url = f"https://twitter.com/{username}/photo"
#     try:
#         response = requests.get(url)
#         response.raise_for_status()  # Levanta una excepciÃ³n si la solicitud falla

#         soup = BeautifulSoup(response.content, 'html.parser')
#         print(soup)

#         # Buscamos la primera imagen dentro de un elemento con clase 'ProfileAvatar'
#         # Si no encontramos esa clase, buscamos la primera imagen en general
#         img_tag = soup.find('img')
#         print(img_tag)

#         if img_tag:
#             return img_tag['src']
#         else:
#             return None

#     except requests.exceptions.RequestException as e:
#         print(f"Error al obtener la imagen de perfil: {e}")
#         return None

# # Ejemplo de uso
# username = "BerlinKozan"
# profile_picture_url = get_twitter_profile_picture(username)
# print(profile_picture_url)